{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# password = _9F33BNH*5AsYt7f4S=J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "from elasticsearch.helpers import bulk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['RecipeId', 'Name', 'AuthorId', 'AuthorName', 'CookTime', 'PrepTime',\n",
      "       'TotalTime', 'DatePublished', 'Description', 'Images', 'RecipeCategory',\n",
      "       'Keywords', 'RecipeIngredientQuantities', 'RecipeIngredientParts',\n",
      "       'AggregatedRating', 'ReviewCount', 'Calories', 'FatContent',\n",
      "       'SaturatedFatContent', 'CholesterolContent', 'SodiumContent',\n",
      "       'CarbohydrateContent', 'FiberContent', 'SugarContent', 'ProteinContent',\n",
      "       'RecipeServings', 'RecipeYield', 'RecipeInstructions'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "csv_file = 'resource/recipes.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"https://img.sndimg.com/food/image/upload/w_555,h_416,c_fit,fl_progressive,q_95/v1/img/submissions/recipe/42265412/fyWEz4cwSf2poDqTo81M_image.jpg\"\n"
     ]
    }
   ],
   "source": [
    "result = df[df['Name'] == 'Cookie Salad'][\"Images\"]\n",
    "print(result.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_type = type(df[df['Name'] == 'Cookie Salad']['Images'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([     5,      8,     10,     13,     15,     19,     22,     23,     24,\n",
      "           25,\n",
      "       ...\n",
      "       522502, 522504, 522505, 522506, 522507, 522508, 522510, 522512, 522514,\n",
      "       522516],\n",
      "      dtype='int64', length=356620)\n"
     ]
    }
   ],
   "source": [
    "indices_character_0 = df[df['Images'] == 'character(0)'].index\n",
    "print(indices_character_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecipeId                           0\n",
      "Name                               0\n",
      "AuthorId                           0\n",
      "AuthorName                         0\n",
      "CookTime                       82545\n",
      "PrepTime                           0\n",
      "TotalTime                          0\n",
      "DatePublished                      0\n",
      "Description                        5\n",
      "Images                             1\n",
      "RecipeCategory                   751\n",
      "Keywords                       17237\n",
      "RecipeIngredientQuantities         3\n",
      "RecipeIngredientParts              0\n",
      "AggregatedRating              253223\n",
      "ReviewCount                   247489\n",
      "Calories                           0\n",
      "FatContent                         0\n",
      "SaturatedFatContent                0\n",
      "CholesterolContent                 0\n",
      "SodiumContent                      0\n",
      "CarbohydrateContent                0\n",
      "FiberContent                       0\n",
      "SugarContent                       0\n",
      "ProteinContent                     0\n",
      "RecipeServings                182911\n",
      "RecipeYield                   348071\n",
      "RecipeInstructions                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print((df.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_nan_values(row):\n",
    "    # Replace NaN values with 0 for numeric columns\n",
    "    for column in row.index:\n",
    "        if isinstance(row[column], (int, float)) and pd.isna(row[column]):\n",
    "            row[column] = 0  # Or any default value you prefer (e.g., \"\")\n",
    "        \n",
    "        # Special case for the RecipeYield column\n",
    "        if column == 'RecipeYield':\n",
    "            # Extract the numeric part from the 'RecipeYield' string (e.g., '4 kebabs' -> 4)\n",
    "            match = re.match(r\"(\\d+)\", str(row[column]))\n",
    "            if match:\n",
    "                row[column] = int(match.group(1))  # Extracted numeric part\n",
    "            else:\n",
    "                row[column] = 0  # If no numeric value, set it to a default (e.g., 0)\n",
    "                \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization took 0.2190 seconds\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 5000 documents: 5000 successful, [] failed\n",
      "Bulk indexed 2517 documents: 2517 successful, [] failed\n",
      "Indexed documents pickled successfully!\n",
      "run_indexer method took 189.3905 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "from elasticsearch.helpers import bulk\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class Indexer:\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.csv_file_path = \"resource/recipes.csv\"\n",
    "        self.es_client = Elasticsearch(\"https://localhost:9200\", \n",
    "                                       basic_auth=(\"elastic\", \"Z_3O+lFyJPcXxPB+UvD-\"), \n",
    "                                       ca_certs=\"~/http_ca.crt\")\n",
    "        self.init_time = time.time() - self.start_time\n",
    "        print(f\"Initialization took {self.init_time:.4f} seconds\")\n",
    "\n",
    "    def run_indexer(self):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Disable refresh for bulk indexing\n",
    "        self.es_client.indices.put_settings(index=\"recipes\", body={\n",
    "            \"settings\": {\n",
    "                \"index\": {\n",
    "                    \"refresh_interval\": \"-1\"  # Disable refresh\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "        # Delete the index if exists and create a new one\n",
    "        self.es_client.options(ignore_status=[400, 404]).indices.delete(index='recipes')\n",
    "        self.es_client.options(ignore_status=400).indices.create(index='recipes')\n",
    "\n",
    "        # Load data from CSV\n",
    "        data = pd.read_csv(self.csv_file_path)\n",
    "\n",
    "        actions = []  # List to hold bulk actions\n",
    "\n",
    "        indexed_documents = []  # List to store documents for pickling\n",
    "\n",
    "        def process_row(row):\n",
    "            row = self.handle_nan_values(row)  # Handle NaN values before creating the document\n",
    "            \n",
    "            document = {\n",
    "                \"_op_type\": \"index\",  # Optional: Index operation for bulk\n",
    "                \"_index\": \"recipes\",  # Index name\n",
    "                \"_id\": row[\"RecipeId\"],  # Optional: If you want to set a custom ID\n",
    "                \"_source\": {\n",
    "                    \"RecipeId\": row[\"RecipeId\"],\n",
    "                    \"Name\": row[\"Name\"],\n",
    "                    \"AuthorId\": row[\"AuthorId\"],\n",
    "                    \"AuthorName\": row[\"AuthorName\"],\n",
    "                    \"CookTime\": row[\"CookTime\"],\n",
    "                    \"PrepTime\": row[\"PrepTime\"],\n",
    "                    \"TotalTime\": row[\"TotalTime\"],\n",
    "                    \"DatePublished\": row[\"DatePublished\"],\n",
    "                    \"Description\": row[\"Description\"],\n",
    "                    \"Images\": row[\"Images\"],\n",
    "                    \"RecipeCategory\": row[\"RecipeCategory\"],\n",
    "                    \"Keywords\": row[\"Keywords\"],\n",
    "                    \"RecipeIngredientQuantities\": row[\"RecipeIngredientQuantities\"],\n",
    "                    \"RecipeIngredientParts\": row[\"RecipeIngredientParts\"],\n",
    "                    \"AggregatedRating\": row[\"AggregatedRating\"],\n",
    "                    \"ReviewCount\": row[\"ReviewCount\"],\n",
    "                    \"Calories\": row[\"Calories\"],\n",
    "                    \"FatContent\": row[\"FatContent\"],\n",
    "                    \"SaturatedFatContent\": row[\"SaturatedFatContent\"],\n",
    "                    \"CholesterolContent\": row[\"CholesterolContent\"],\n",
    "                    \"SodiumContent\": row[\"SodiumContent\"],\n",
    "                    \"CarbohydrateContent\": row[\"CarbohydrateContent\"],\n",
    "                    \"FiberContent\": row[\"FiberContent\"],\n",
    "                    \"SugarContent\": row[\"SugarContent\"],\n",
    "                    \"ProteinContent\": row[\"ProteinContent\"],\n",
    "                    \"RecipeServings\": row[\"RecipeServings\"],\n",
    "                    \"RecipeYield\": row[\"RecipeYield\"],\n",
    "                    \"RecipeInstructions\": row[\"RecipeInstructions\"]\n",
    "                }\n",
    "            }\n",
    "            return document\n",
    "\n",
    "        # Use ThreadPoolExecutor to parallelize the row processing\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            documents = list(executor.map(process_row, [row for idx, row in data.iterrows()]))\n",
    "        \n",
    "        # Perform bulk indexing in batches of 5000 or a suitable number\n",
    "        for i in range(0, len(documents), 5000):\n",
    "            batch = documents[i:i + 5000]\n",
    "            success, failed = bulk(self.es_client, batch)\n",
    "            print(f\"Bulk indexed {len(batch)} documents: {success} successful, {failed} failed\")\n",
    "        \n",
    "        # Enable refresh again after bulk indexing\n",
    "        self.es_client.indices.put_settings(index=\"recipes\", body={\n",
    "            \"settings\": {\n",
    "                \"index\": {\n",
    "                    \"refresh_interval\": \"1s\"  # Re-enable refresh interval\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "        # Pickle the indexed documents to a file\n",
    "        with open('resource/recipes_index.pkl', 'wb') as f:\n",
    "            pickle.dump(documents, f)\n",
    "        print(\"Indexed documents pickled successfully!\")\n",
    "\n",
    "        end_time = time.time() - start_time\n",
    "        print(f\"run_indexer method took {end_time:.4f} seconds\")\n",
    "\n",
    "    def handle_nan_values(self, row):\n",
    "        # Replace NaN values with 0 for numeric columns\n",
    "        for column in row.index:\n",
    "            if isinstance(row[column], (int, float)) and pd.isna(row[column]):\n",
    "                row[column] = 0  # Or any default value you prefer (e.g., \"\")\n",
    "            \n",
    "            # Special case for the RecipeYield column\n",
    "            if column == 'RecipeYield':\n",
    "                # Extract the numeric part from the 'RecipeYield' string (e.g., '4 kebabs' -> 4)\n",
    "                match = re.match(r\"(\\d+)\", str(row[column]))\n",
    "                if match:\n",
    "                    row[column] = int(match.group(1))  # Extracted numeric part\n",
    "                else:\n",
    "                    row[column] = 0  # If no numeric value, set it to a default (e.g., 0)\n",
    "                \n",
    "        return row\n",
    "\n",
    "\n",
    "# Run the indexing process\n",
    "indexer = Indexer()\n",
    "indexer.run_indexer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, jsonify\n",
    "from elasticsearch import Elasticsearch\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Initialize Elasticsearch client\n",
    "es = Elasticsearch(\n",
    "    \"https://localhost:9200\", \n",
    "    basic_auth=(\"elastic\", os.getenv('ELASTIC_PASSWORD', 'Z_3O+lFyJPcXxPB+UvD-')),  # Using environment variable for security\n",
    "    ca_certs=os.path.expanduser(\"~/http_ca.crt\")\n",
    "    ,verify_certs=False\n",
    " # Ensure the path is correct\n",
    ")\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/search', methods=['GET'])\n",
    "def search():\n",
    "    query = request.args.get('q', '')\n",
    "    if query:\n",
    "        # Perform Elasticsearch search\n",
    "        response = es.search(index=\"recipes\", body={\n",
    "            \"query\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": query,\n",
    "                    \"fields\": [\"Name\", \"Description\", \"RecipeInstructions\"],\n",
    "                }\n",
    "            },\n",
    "            \"size\": 1000  # Increase the size to get a large enough batch of results\n",
    "        })\n",
    "\n",
    "        results = []\n",
    "        seen_recipe_ids = set()  # Set to track unique RecipeIds\n",
    "\n",
    "        for hit in response['hits']['hits']:\n",
    "            recipe_id = hit['_source'].get('RecipeId')\n",
    "            if recipe_id not in seen_recipe_ids:\n",
    "                results.append(hit['_source'])\n",
    "                seen_recipe_ids.add(recipe_id)  # Mark this RecipeId as seen\n",
    "\n",
    "        if results:\n",
    "            return jsonify(results)\n",
    "        else:\n",
    "            return jsonify({\"message\": \"No results found\"}), 404  # Return message if no results found\n",
    "    return jsonify({\"message\": \"No query provided\"}), 400  # Return message if no query is provided\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "c:\\Users\\user\\miniconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [25/Feb/2025 21:31:25] \"GET /search?q=hello%20there HTTP/1.1\" 200 -\n",
      "c:\\Users\\user\\miniconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [25/Feb/2025 21:31:26] \"GET /search?q=hello%20there HTTP/1.1\" 200 -\n",
      "c:\\Users\\user\\miniconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [25/Feb/2025 21:31:26] \"GET /search?q=hello%20there HTTP/1.1\" 200 -\n",
      "c:\\Users\\user\\miniconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [25/Feb/2025 21:31:27] \"GET /search?q=hello%20there HTTP/1.1\" 200 -\n",
      "c:\\Users\\user\\miniconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [25/Feb/2025 21:31:31] \"GET /search?q=hello HTTP/1.1\" 200 -\n",
      "c:\\Users\\user\\miniconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [25/Feb/2025 21:31:33] \"GET /search?q=hello HTTP/1.1\" 200 -\n",
      "c:\\Users\\user\\miniconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [25/Feb/2025 21:31:39] \"GET /search?q=panna%20cotta HTTP/1.1\" 200 -\n",
      "c:\\Users\\user\\miniconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [25/Feb/2025 21:45:51] \"GET /search?q=Velvet%20Mousse HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
