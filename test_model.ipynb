{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[150], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull_recipes.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m review \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresource/reviews.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:331\u001b[0m, in \u001b[0;36mgetstate\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('full_recipes.csv')\n",
    "review = pd.read_csv('resource/reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['RecipeId', 'AggregatedRating', 'ReviewCount', 'Calories', 'FatContent', 'SaturatedFatContent',\n",
    "            'CholesterolContent', 'SodiumContent', 'CarbohydrateContent', 'FiberContent', 'SugarContent',\n",
    "            'ProteinContent', 'RecipeServings', 'RecipeCategory']\n",
    "\n",
    "recipes = df[features]\n",
    "merged_df = recipes.merge(review, left_on='RecipeId', right_on='ReviewId', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ReviewId', 'RecipeId', 'AuthorId', 'AuthorName', 'Rating', 'Review',\n",
       "       'DateSubmitted', 'DateModified'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = merged_df.dropna(subset=['RecipeCategory'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(columns=['AuthorName','AuthorId','RecipeId_y','DateSubmitted','DateModified','Review'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   < 15 Mins  < 30 Mins  < 4 Hours  < 60 Mins  African  Apple  Apple Pie  \\\n",
      "0      False      False      False      False    False  False      False   \n",
      "1      False      False      False      False    False  False      False   \n",
      "2      False      False      False      False    False  False      False   \n",
      "3      False      False      False      False    False  False      False   \n",
      "4      False      False      False      False    False  False      False   \n",
      "\n",
      "   Artichoke  Asian  Australian  ...  Wheat Bread  White Rice  Whitefish  \\\n",
      "0      False  False       False  ...        False       False      False   \n",
      "1      False  False       False  ...        False       False      False   \n",
      "2      False  False       False  ...        False       False      False   \n",
      "3      False  False       False  ...        False       False      False   \n",
      "4      False  False       False  ...        False       False      False   \n",
      "\n",
      "   Whole Chicken  Whole Duck  Whole Turkey  Wild Game  Winter  \\\n",
      "0          False       False         False      False   False   \n",
      "1          False       False         False      False   False   \n",
      "2          False       False         False      False   False   \n",
      "3          False       False         False      False   False   \n",
      "4          False       False         False      False   False   \n",
      "\n",
      "   Yam/Sweet Potato  Yeast Breads  \n",
      "0             False         False  \n",
      "1             False         False  \n",
      "2             False         False  \n",
      "3             False         False  \n",
      "4             False         False  \n",
      "\n",
      "[5 rows x 308 columns]\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode using pandas\n",
    "encoded_df = pd.get_dummies(merged_df['RecipeCategory'], drop_first=False)\n",
    "\n",
    "# The result will already have 1 and 0 instead of True and False\n",
    "print(encoded_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['< 15 Mins', '< 30 Mins', '< 4 Hours', '< 60 Mins', 'African', 'Apple',\n",
       "       'Apple Pie', 'Artichoke', 'Asian', 'Australian',\n",
       "       ...\n",
       "       'Wheat Bread', 'White Rice', 'Whitefish', 'Whole Chicken', 'Whole Duck',\n",
       "       'Whole Turkey', 'Wild Game', 'Winter', 'Yam/Sweet Potato',\n",
       "       'Yeast Breads'],\n",
       "      dtype='object', length=308)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "# Apply SimpleImputer only on the selected columns\n",
    "imputed_values = imputer.fit_transform(merged_df[['AggregatedRating', 'ReviewCount', 'RecipeServings']])\n",
    "\n",
    "# Assign the imputed values back to the original columns in merged_df\n",
    "merged_df[['AggregatedRating', 'ReviewCount', 'RecipeServings']] = imputed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_features = merged_df['RecipeCategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecipeId_x               0\n",
      "AggregatedRating         0\n",
      "ReviewCount              0\n",
      "Calories                 0\n",
      "FatContent               0\n",
      "SaturatedFatContent      0\n",
      "CholesterolContent       0\n",
      "SodiumContent            0\n",
      "CarbohydrateContent      0\n",
      "FiberContent             0\n",
      "SugarContent             0\n",
      "ProteinContent           0\n",
      "RecipeServings           0\n",
      "RecipeCategory         688\n",
      "ReviewId                 0\n",
      "Rating                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# merged_df.dropna(subset=['RecipeCategory'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'RecipeCategory' column from the original DataFrame\n",
    "merged_df = merged_df.drop(columns=['RecipeCategory'])\n",
    "merged_df = pd.concat([merged_df, encoded_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit, blindtest = train_test_split(merged_df, test_size=0.2, random_state=0)\n",
    "fit_train, fit_test = train_test_split(fit, test_size=0.3, random_state=0)\n",
    "features +=  df['RecipeCategory'].unique().tolist()\n",
    "review_col = 'ReviewId'\n",
    "recipe_col = 'RecipeId_x'\n",
    "target_col = 'Rating'\n",
    "\n",
    "fit_train = fit_train.sort_values('ReviewId').reset_index(drop=True)\n",
    "fit_test = fit_test.sort_values('ReviewId').reset_index(drop=True)\n",
    "blindtest = blindtest.sort_values('ReviewId').reset_index(drop=True)\n",
    "\n",
    "fit_train_query = fit_train[review_col].value_counts().sort_index()\n",
    "fit_test_query = fit_test[review_col].value_counts().sort_index()\n",
    "blindtest_query = blindtest[review_col].value_counts().sort_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict({'ndcg@1': 1.0, 'ndcg@3': 1.0, 'ndcg@5': 1.0, 'ndcg@10': 1.0})})\n",
      "Best NDCG@1: 1.0\n",
      "Best NDCG@3: 1.0\n",
      "Best NDCG@5: 1.0\n",
      "Best NDCG@10: 1.0\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMRanker(n_estimators=1000, random_state=0)\n",
    "model.fit(\n",
    "    fit_train,\n",
    "    fit_train[target_col],\n",
    "    group=fit_train_query,\n",
    "    eval_set=[(fit_test, fit_test[target_col])],\n",
    "    eval_group=[list(fit_test_query)],\n",
    "    eval_at=[1, 3, 5, 10], \n",
    ")\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(fit_test)\n",
    "\n",
    "# Evaluate the model using the metrics provided by LightGBM\n",
    "eval_results = model.best_score_  # Get the evaluation results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       RecipeId_x  PredictedRanking\n",
      "0              42               0.0\n",
      "77223      356048               0.0\n",
      "77222      356045               0.0\n",
      "77221      356044               0.0\n",
      "77220      356043               0.0\n",
      "77219      356032               0.0\n",
      "77218      356030               0.0\n",
      "77217      356026               0.0\n",
      "77216      356014               0.0\n",
      "77215      356012               0.0\n"
     ]
    }
   ],
   "source": [
    "fit_test['PredictedRanking'] = y_pred\n",
    "result_df = fit_test[['RecipeId_x', 'PredictedRanking']]\n",
    "\n",
    "# Sort by predicted ranking\n",
    "result_df_sorted = result_df.sort_values(by='PredictedRanking', ascending=True)\n",
    "\n",
    "# Display the top 10 predictions\n",
    "top_predictions = result_df_sorted.head(10)\n",
    "print(top_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset = fit_test.sample(frac=0.5, random_state=0)\n",
    "# Predict on the subset of the test set\n",
    "test_features = fit.columns\n",
    "y_pred = model.predict(test_subset[test_features])\n",
    "\n",
    "# Add the predictions to the test subset\n",
    "test_subset['PredictedRating'] = y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       RecipeId_x  PredictedRating  Rating\n",
      "82850      381468              0.0       4\n",
      "8086        41429              0.0       5\n",
      "26263      124146              0.0       4\n",
      "15285       74590              0.0       5\n",
      "51983      240830              0.0       4\n",
      "58261      270029              0.0       5\n",
      "70534      325871              0.0       5\n",
      "96075      446199              0.0       5\n",
      "41930      194965              0.0       4\n",
      "64032      295741              0.0       5\n"
     ]
    }
   ],
   "source": [
    "# Sort by predicted ratings and display the top results\n",
    "test_subset_sorted = test_subset.sort_values(by='PredictedRating', ascending=False)\n",
    "\n",
    "# Display top 10 rows with 'RecipeId_x', 'PredictedRating', and actual 'Rating'\n",
    "print(test_subset_sorted[['RecipeId_x', 'PredictedRating', target_col]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        RecipeId                    Name  AuthorId AuthorName CookTime  \\\n",
      "231192    240830  Scallop and Bacon Toss    137911   Pam-I-Am    PT15M   \n",
      "\n",
      "       PrepTime TotalTime         DatePublished  \\\n",
      "231192    PT15M     PT30M  2007-07-16T22:30:00Z   \n",
      "\n",
      "                                              Description RecipeCategory  ...  \\\n",
      "231192  Make and share this Scallop and Bacon Toss rec...  One Dish Meal  ...   \n",
      "\n",
      "       SodiumContent CarbohydrateContent FiberContent  SugarContent  \\\n",
      "231192         222.7                32.2          2.0           1.9   \n",
      "\n",
      "        ProteinContent  RecipeServings  RecipeYield  \\\n",
      "231192            16.8             6.0          NaN   \n",
      "\n",
      "                                       RecipeInstructions  \\\n",
      "231192  c(\"Cook vermicelli as package directs drain.\",...   \n",
      "\n",
      "                                                   Images  \\\n",
      "231192  c(\"https://img.sndimg.com/food/image/upload/w_...   \n",
      "\n",
      "                                                     text  \n",
      "231192  Scallop and Bacon Toss c(\"bacon\", \"green bell ...  \n",
      "\n",
      "[1 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "# List of RecipeId_x values to filter\n",
    "recipe_ids = [240830]\n",
    "\n",
    "# Filter the DataFrame based on RecipeId_x\n",
    "filtered_df = df[df['RecipeId'].isin(recipe_ids)]\n",
    "\n",
    "# Print the filtered rows\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
