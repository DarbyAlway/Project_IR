{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.read_csv('full_recipes.csv')\n",
    "test_data = pd.read_csv('for_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (522517, 29) , test_data (2, 28)\n"
     ]
    }
   ],
   "source": [
    "print(f'data shape: {data.shape} , test_data { test_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['RecipeId', 'AggregatedRating', 'ReviewCount', 'Calories', 'FatContent', 'SaturatedFatContent',\n",
    "            'CholesterolContent', 'SodiumContent', 'CarbohydrateContent', 'FiberContent', 'SugarContent',\n",
    "            'ProteinContent', 'RecipeServings', 'RecipeCategory']\n",
    "data = data[features]\n",
    "test_data = test_data[features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "\n",
    "data['RecipeCategory']= encoder.fit_transform(data['RecipeCategory'])\n",
    "test_data['RecipeCategory'] = encoder.transform(test_data['RecipeCategory'])\n",
    "\n",
    "# encoded_data.drop(columns='RecipeCategory',inplace=True)\n",
    "# encoded_test_data.drop(columns='RecipeCategory',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (522517, 14) , test_data (2, 14)\n"
     ]
    }
   ],
   "source": [
    "print(f'data shape: {data.shape} , test_data { test_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_col = ['AggregatedRating', 'ReviewCount', 'RecipeServings']\n",
    "\n",
    "# Initialize the imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit and transform the data\n",
    "data_imputed = pd.DataFrame(imputer.fit_transform(data[null_col]), columns=null_col, index=data.index)\n",
    "test_data_imputed = pd.DataFrame(imputer.transform(test_data[null_col]), columns=null_col, index=test_data.index)\n",
    "\n",
    "# Replace original columns with imputed values\n",
    "data[null_col] = data_imputed\n",
    "test_data[null_col] = test_data_imputed\n",
    "\n",
    "data = data.drop(columns=['RecipeCategory'])\n",
    "test_data = test_data.drop(columns=['RecipeCategory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"imputer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(imputer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (522517, 13) , test_data (2, 13)\n"
     ]
    }
   ],
   "source": [
    "print(f'data shape: {data.shape} , test_data { test_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit, blindtest = train_test_split(data, test_size=0.2, random_state=0)\n",
    "fit_train, fit_test = train_test_split(fit, test_size=0.3, random_state=0)\n",
    "recipe_col = 'RecipeId'\n",
    "target_col = 'AggregatedRating'\n",
    "\n",
    "fit_train = fit_train.sort_values('RecipeId').reset_index(drop=True)\n",
    "fit_test = fit_test.sort_values('RecipeId').reset_index(drop=True)\n",
    "blindtest = blindtest.sort_values('RecipeId').reset_index(drop=True)\n",
    "\n",
    "fit_train_query = fit_train[recipe_col].value_counts().sort_index()\n",
    "fit_test_query = fit_test[recipe_col].value_counts().sort_index()\n",
    "blindtest_query = blindtest[recipe_col].value_counts().sort_index()\n",
    "\n",
    "fit_train_query = list(map(int, fit_train_query))\n",
    "fit_test_query = list(map(int, fit_test_query))\n",
    "\n",
    "fit_train[target_col] = fit_train[target_col].astype(int)\n",
    "fit_test[target_col] = fit_test[target_col].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LightGBMError",
     "evalue": "label should be int type (met 4.500000) for ranking task,\nfor the gain of label, please set the label_gain parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[223], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMRanker(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_train_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfit_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfit_test_query\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_at\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(fit_test)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Evaluate the model using the metrics provided by LightGBM\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:1780\u001b[0m, in \u001b[0;36mLGBMRanker.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_group, eval_metric, eval_at, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1774\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1775\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould set group for all eval datasets for ranking task; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1776\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif you use dict, the index should start from 0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1777\u001b[0m         )\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_at \u001b[38;5;241m=\u001b[39m eval_at\n\u001b[1;32m-> 1780\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1782\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1783\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1784\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1786\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1787\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1788\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1789\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1790\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1791\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1795\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1796\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1797\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:1049\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1046\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1047\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m-> 1049\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mnum_feature()\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\Lib\\site-packages\\lightgbm\\engine.py:297\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 297\u001b[0m     booster \u001b[38;5;241m=\u001b[39m \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    299\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\Lib\\site-packages\\lightgbm\\basic.py:3660\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[1;34m(self, params, train_set, model_file, model_str)\u001b[0m\n\u001b[0;32m   3658\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n\u001b[0;32m   3659\u001b[0m params_str \u001b[38;5;241m=\u001b[39m _param_dict_to_str(params)\n\u001b[1;32m-> 3660\u001b[0m \u001b[43m_safe_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3661\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterCreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3663\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_c_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3665\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3666\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3667\u001b[0m \u001b[38;5;66;03m# save reference to data\u001b[39;00m\n\u001b[0;32m   3668\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_set \u001b[38;5;241m=\u001b[39m train_set\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\Lib\\site-packages\\lightgbm\\basic.py:313\u001b[0m, in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m    The return value from C API calls.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(_LIB\u001b[38;5;241m.\u001b[39mLGBM_GetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mLightGBMError\u001b[0m: label should be int type (met 4.500000) for ranking task,\nfor the gain of label, please set the label_gain parameter"
     ]
    }
   ],
   "source": [
    "\n",
    "model = lgb.LGBMRanker(n_estimators=1000, random_state=0)\n",
    "model.fit(\n",
    "    fit_train,\n",
    "    fit_train[target_col],\n",
    "    group=fit_train_query,\n",
    "    eval_set=[(fit_test, fit_test[target_col])],\n",
    "    eval_group=[list(fit_test_query)],\n",
    "    eval_at=[1, 3, 5, 10], \n",
    ")\n",
    "\n",
    "y_pred = model.predict(fit_test)\n",
    "\n",
    "# Evaluate the model using the metrics provided by LightGBM\n",
    "eval_results = model.best_score_  # Get the evaluation results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Estimator not fitted, call fit before exploiting the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[221], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Predict ratings on the test set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfit_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Add predictions to the test set\u001b[39;00m\n\u001b[0;32m      5\u001b[0m fit_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_rating\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m predictions\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:1106\u001b[0m, in \u001b[0;36mLGBMModel.predict\u001b[1;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is set after definition, using a template.\"\"\"\u001b[39;00m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__sklearn_is_fitted__():\n\u001b[1;32m-> 1106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LGBMNotFittedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimator not fitted, call fit before exploiting the model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, (pd_DataFrame, dt_DataTable)):\n\u001b[0;32m   1108\u001b[0m     X \u001b[38;5;241m=\u001b[39m _LGBMValidateData(\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1110\u001b[0m         X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1121\u001b[0m         ensure_min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1122\u001b[0m     )\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Estimator not fitted, call fit before exploiting the model."
     ]
    }
   ],
   "source": [
    "# Predict ratings on the test set\n",
    "predictions = model.predict(fit_test)\n",
    "\n",
    "# Add predictions to the test set\n",
    "fit_test['predicted_rating'] = predictions\n",
    "\n",
    "# Sort the test set by the predicted ratings in descending order\n",
    "sorted_test = fit_test.sort_values('predicted_rating', ascending=False)\n",
    "\n",
    "# Get the top `RecipeId` with the highest predicted ratings\n",
    "top_recipe_id = sorted_test['RecipeId'].iloc[0]  # This gives the `RecipeId` with the highest rating\n",
    "\n",
    "print(f\"The RecipeId with the highest predicted rating is: {top_recipe_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Save the model to a file\n",
    "with open(\"new_lgbm_ranker.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       RecipeId  PredictedRanking\n",
      "0            47               0.0\n",
      "83610    361957               0.0\n",
      "83609    361953               0.0\n",
      "83608    361952               0.0\n",
      "83607    361950               0.0\n",
      "83606    361942               0.0\n",
      "83605    361939               0.0\n",
      "83604    361937               0.0\n",
      "83603    361936               0.0\n",
      "83602    361934               0.0\n"
     ]
    }
   ],
   "source": [
    "fit_test['PredictedRanking'] = y_pred\n",
    "result_df = fit_test[['RecipeId', 'PredictedRanking']]\n",
    "\n",
    "# Sort by predicted ranking\n",
    "result_df_sorted = result_df.sort_values(by='PredictedRanking', ascending=True)\n",
    "\n",
    "# Display the top 10 predictions\n",
    "top_predictions = result_df_sorted.head(10)\n",
    "print(top_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_test['PredictedRanking'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        RecipeId  PredictedRating  AggregatedRating\n",
      "120998    522378              0.0                 4\n",
      "118148    510258              0.0                 4\n",
      "107415    463991              0.0                 4\n",
      "1178        7731              0.0                 4\n",
      "48873     212042              0.0                 4\n",
      "114242    493621              0.0                 4\n",
      "96729     418111              0.0                 4\n",
      "82709     357876              0.0                 4\n",
      "46333     201087              0.0                 4\n",
      "96304     416308              0.0                 5\n"
     ]
    }
   ],
   "source": [
    "test_subset = fit_test.sample(frac=0.5, random_state=0)\n",
    "# Predict on the subset of the test set\n",
    "test_features = fit.columns\n",
    "y_pred = model.predict(test_subset[test_features])\n",
    "\n",
    "# Add the predictions to the test subset\n",
    "test_subset['PredictedRating'] = y_pred\n",
    "# Sort by predicted ratings and display the top results\n",
    "test_subset_sorted = test_subset.sort_values(by='PredictedRating', ascending=False)\n",
    "\n",
    "# Display top 10 rows with 'RecipeId_x', 'PredictedRating', and actual 'Rating'\n",
    "print(test_subset_sorted[['RecipeId', 'PredictedRating', target_col]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    \"\"\"Load a pickled object if it exists, otherwise return None.\"\"\"\n",
    "    if os.path.exists(file_name):\n",
    "        with open(file_name, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def save_pickle(obj, file_name):\n",
    "    \"\"\"Save an object to a pickle file.\"\"\"\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def preprocess_data(df, fit_encoder=True, fit_imputer=True):\n",
    "    features = ['RecipeId', 'AggregatedRating', 'ReviewCount', 'Calories', 'FatContent', 'SaturatedFatContent',\n",
    "                'CholesterolContent', 'SodiumContent', 'CarbohydrateContent', 'FiberContent', 'SugarContent',\n",
    "                'ProteinContent', 'RecipeServings', 'RecipeCategory']\n",
    "    \n",
    "    df = df[features].copy()  # Ensure a copy to avoid modifying the original DataFrame\n",
    "    \n",
    "    # Load or create LabelEncoder\n",
    "    encoder = load_pickle(\"encoder.pkl\") if not fit_encoder else LabelEncoder()\n",
    "    \n",
    "    # Encode 'RecipeCategory'\n",
    "    if fit_encoder:\n",
    "        df['RecipeCategory'] = encoder.fit_transform(df['RecipeCategory'])\n",
    "        save_pickle(encoder, \"encoder.pkl\")  # Save the fitted encoder\n",
    "    else:\n",
    "        df['RecipeCategory'] = encoder.transform(df['RecipeCategory'])\n",
    "    \n",
    "    # Select columns for imputation\n",
    "    null_col = ['AggregatedRating', 'ReviewCount', 'RecipeServings']\n",
    "    \n",
    "    # Load or create SimpleImputer\n",
    "    imputer = load_pickle(\"imputer.pkl\") if not fit_imputer else SimpleImputer(strategy='mean')\n",
    "    \n",
    "    # Fit and transform for training, only transform for test data\n",
    "    if fit_imputer:\n",
    "        imputed_values = imputer.fit_transform(df[null_col])\n",
    "        save_pickle(imputer, \"imputer.pkl\")  # Save the fitted imputer\n",
    "    else:\n",
    "        imputed_values = imputer.transform(df[null_col])\n",
    "    \n",
    "    df[null_col] = pd.DataFrame(imputed_values, columns=null_col, index=df.index)\n",
    "    \n",
    "    # Drop 'RecipeCategory'\n",
    "    df = df.drop(columns=['RecipeCategory'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "test_data_2 = pd.read_csv('for_model.csv')\n",
    "# Test data (using saved encoder and imputer)\n",
    "test_data = preprocess_data(test_data_2, fit_encoder=False, fit_imputer=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Estimator not fitted, call fit before exploiting the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[222], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Predict ratings on the test set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Add predictions to the test set\u001b[39;00m\n\u001b[0;32m      5\u001b[0m fit_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_rating\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m predictions\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\Lib\\site-packages\\lightgbm\\sklearn.py:1106\u001b[0m, in \u001b[0;36mLGBMModel.predict\u001b[1;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is set after definition, using a template.\"\"\"\u001b[39;00m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__sklearn_is_fitted__():\n\u001b[1;32m-> 1106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LGBMNotFittedError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimator not fitted, call fit before exploiting the model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, (pd_DataFrame, dt_DataTable)):\n\u001b[0;32m   1108\u001b[0m     X \u001b[38;5;241m=\u001b[39m _LGBMValidateData(\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1110\u001b[0m         X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1121\u001b[0m         ensure_min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1122\u001b[0m     )\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Estimator not fitted, call fit before exploiting the model."
     ]
    }
   ],
   "source": [
    "# Predict ratings on the test set\n",
    "predictions = model.predict(y_pred)\n",
    "\n",
    "# Add predictions to the test set\n",
    "fit_test['predicted_rating'] = predictions\n",
    "\n",
    "# Sort the test set by the predicted ratings in descending order\n",
    "sorted_test = fit_test.sort_values('predicted_rating', ascending=False)\n",
    "\n",
    "# Get the top `RecipeId` with the highest predicted ratings\n",
    "top_recipe_id = sorted_test['RecipeId'].iloc[0]  # This gives the `RecipeId` with the highest rating\n",
    "\n",
    "print(f\"The RecipeId with the highest predicted rating is: {top_recipe_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
